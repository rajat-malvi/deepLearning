{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdJw/VOO1Di9y8sTV00uDg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajat-malvi/deepLearning/blob/main/NN_Train_and_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nural Network train"
      ],
      "metadata": {
        "id": "CNpTGSU89_Sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# require imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "import time\n",
        "from collections import OrderedDict\n"
      ],
      "metadata": {
        "id": "D31Val4wADe2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataSet need to be used\n",
        "datasets_to_run = [\"cifar10\", \"mnist\", \"fashion_mnist\", \"stl10\", \"svhn\"]"
      ],
      "metadata": {
        "id": "TbWBXmb4BKrO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define arguments directly for Colab execution\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 128\n",
        "LR = 0.01\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "HWH1akrdALMy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sf8Sab1HBKCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader\n",
        "def get_dataloader(name, train=True, batch_size=128, num_workers=4):\n",
        "    \"\"\"\n",
        "    Returns (dataloader, num_classes, in_channels)\n",
        "    name: one of 'cifar10','mnist','fashion_mnist','stl10','svhn'\n",
        "    \"\"\"\n",
        "    name = name.lower()\n",
        "    # default sizes/in_channels\n",
        "    if name in (\"mnist\", \"fashion_mnist\"):\n",
        "        in_channels = 1\n",
        "        size = 32\n",
        "    else:\n",
        "        in_channels = 3\n",
        "        size = 32\n",
        "\n",
        "    # transforms: normalize to approx ranges used commonly\n",
        "    common_transforms = []\n",
        "    # For STL-10 images are 96x96, resize to 32 for model simplicity\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Add a simple normalization per channel for stability\n",
        "    # For grayscale datasets use single channel mean/std\n",
        "    if in_channels == 1:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    else:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "        ])\n",
        "\n",
        "    if name == \"cifar10\":\n",
        "        ds = datasets.CIFAR10(root=\"./data\", train=train, transform=transform, download=True)\n",
        "        num_classes = 10\n",
        "    elif name == \"mnist\":\n",
        "        ds = datasets.MNIST(root=\"./data\", train=train, transform=transform, download=True)\n",
        "        num_classes = 10\n",
        "    elif name == \"fashion_mnist\" or name == \"fashion-mnist\" or name == \"fashionmnist\":\n",
        "        ds = datasets.FashionMNIST(root=\"./data\", train=train, transform=transform, download=True)\n",
        "        num_classes = 10\n",
        "    elif name == \"stl10\" or name == \"slt-10\" or name == \"slt10\":\n",
        "        # torchvision's STL10 has splits 'train' and 'test' and 'unlabeled'\n",
        "        split = \"train\" if train else \"test\"\n",
        "        ds = datasets.STL10(root=\"./data\", split=split, transform=transform, download=True)\n",
        "        num_classes = 10\n",
        "    elif name == \"svhn\" or name == \"svhm\" or name == \"svhmn\":\n",
        "        split = \"train\" if train else \"test\"\n",
        "        ds = datasets.SVHN(root=\"./data\", split=split, transform=transform, download=True)\n",
        "        num_classes = 10\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {name}\")\n",
        "\n",
        "    shuffle = True if train else False\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
        "    return loader, num_classes, in_channels"
      ],
      "metadata": {
        "id": "kMkMNXRFAT4K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nural network with convolution\n",
        "class GenericCNN(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10, base_channels=32):\n",
        "        super().__init__()\n",
        "        c = base_channels\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            (\"conv1\", nn.Conv2d(in_channels, c, kernel_size=3, padding=1)),\n",
        "            (\"bn1\", nn.BatchNorm2d(c)),\n",
        "            (\"relu1\", nn.ReLU(inplace=True)),\n",
        "            (\"pool1\", nn.MaxPool2d(2)),  # 32->16\n",
        "\n",
        "            (\"conv2\", nn.Conv2d(c, c*2, kernel_size=3, padding=1)),\n",
        "            (\"bn2\", nn.BatchNorm2d(c*2)),\n",
        "            (\"relu2\", nn.ReLU(inplace=True)),\n",
        "            (\"pool2\", nn.MaxPool2d(2)),  # 16->8\n",
        "\n",
        "            (\"conv3\", nn.Conv2d(c*2, c*4, kernel_size=3, padding=1)),\n",
        "            (\"bn3\", nn.BatchNorm2d(c*4)),\n",
        "            (\"relu3\", nn.ReLU(inplace=True)),\n",
        "            (\"pool3\", nn.AdaptiveAvgPool2d((1,1)))  # -> 1x1\n",
        "        ]))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(c*4*1*1, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uCTS5dD7Abg6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "-47PESPRA6Hx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Evaluation loops\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    loss = running_loss / total\n",
        "    acc = 100.0 * correct / total\n",
        "    return loss, acc\n"
      ],
      "metadata": {
        "id": "lDZm7fT5BBRv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training_for(name):\n",
        "    print(\"-\"*10,f\"Dataset: {name}\", \"-\"*10)\n",
        "    train_loader, num_classes, in_channels = get_dataloader(name, train=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "    test_loader, _, _ = get_dataloader(name, train=False, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "\n",
        "    model = GenericCNN(in_channels=in_channels, num_classes=num_classes).to(DEVICE)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_test_acc = 0.0\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)\n",
        "        scheduler.step()\n",
        "        t1 = time.time()\n",
        "        print(f\"Epoch {epoch:02d}/{EPOCHS} | time: {t1-t0:.1f}s | train_loss: {train_loss:.4f} train_acc: {train_acc:.2f}% | test_loss: {test_loss:.4f} test_acc: {test_acc:.2f}%\")\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "    print(f\"Best test accuracy for {name}: {best_test_acc:.2f}%\")\n",
        "    return best_test_acc\n"
      ],
      "metadata": {
        "id": "n6oE5MXRB5pq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculation\n",
        "results = {}\n",
        "start_all = time.time()\n",
        "for ds_name in datasets_to_run:\n",
        "    try:\n",
        "        acc = run_training_for(ds_name)\n",
        "        results[ds_name] = acc\n",
        "    except Exception as e:\n",
        "        print(f\"Error while training {ds_name}: {e}\")\n",
        "        results[ds_name] = None\n",
        "\n",
        "total_time = time.time() - start_all\n",
        "print(f\"\\nAll done. Total time: {total_time/60:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "m_KLFK7fCrox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad861aa-8278-4abe-e1bb-5f69855e4e04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Dataset: cifar10 ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 47.7MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/8 | time: 17.5s | train_loss: 1.6994 train_acc: 36.30% | test_loss: 1.4724 test_acc: 45.83%\n",
            "Epoch 02/8 | time: 15.1s | train_loss: 1.3264 train_acc: 51.81% | test_loss: 1.4188 test_acc: 48.11%\n",
            "Epoch 03/8 | time: 15.1s | train_loss: 1.1888 train_acc: 57.09% | test_loss: 1.3616 test_acc: 51.73%\n",
            "Epoch 04/8 | time: 15.2s | train_loss: 1.0946 train_acc: 60.60% | test_loss: 1.0977 test_acc: 59.93%\n",
            "Epoch 05/8 | time: 15.2s | train_loss: 1.0198 train_acc: 63.36% | test_loss: 1.0083 test_acc: 63.93%\n",
            "Epoch 06/8 | time: 16.7s | train_loss: 0.9712 train_acc: 65.52% | test_loss: 1.0794 test_acc: 61.44%\n",
            "Epoch 07/8 | time: 15.6s | train_loss: 0.9258 train_acc: 67.04% | test_loss: 1.1474 test_acc: 59.04%\n",
            "Epoch 08/8 | time: 15.1s | train_loss: 0.8135 train_acc: 71.48% | test_loss: 0.8124 test_acc: 70.85%\n",
            "Best test accuracy for cifar10: 70.85%\n",
            "---------- Dataset: mnist ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.0MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 487kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.49MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.85MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/8 | time: 19.1s | train_loss: 0.9994 train_acc: 67.71% | test_loss: 0.2974 test_acc: 91.92%\n",
            "Epoch 02/8 | time: 17.4s | train_loss: 0.1967 train_acc: 94.74% | test_loss: 0.7649 test_acc: 73.51%\n",
            "Epoch 03/8 | time: 16.6s | train_loss: 0.1230 train_acc: 96.62% | test_loss: 0.1655 test_acc: 94.78%\n",
            "Epoch 04/8 | time: 18.2s | train_loss: 0.0969 train_acc: 97.26% | test_loss: 0.0944 test_acc: 97.16%\n",
            "Epoch 05/8 | time: 17.6s | train_loss: 0.0816 train_acc: 97.73% | test_loss: 0.0891 test_acc: 97.29%\n",
            "Epoch 06/8 | time: 17.6s | train_loss: 0.0702 train_acc: 98.07% | test_loss: 0.3375 test_acc: 88.10%\n",
            "Epoch 07/8 | time: 17.4s | train_loss: 0.0653 train_acc: 98.11% | test_loss: 0.0785 test_acc: 97.61%\n",
            "Epoch 08/8 | time: 16.8s | train_loss: 0.0482 train_acc: 98.64% | test_loss: 0.0412 test_acc: 98.80%\n",
            "Best test accuracy for mnist: 98.80%\n",
            "---------- Dataset: fashion_mnist ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 13.3MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 211kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.89MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/8 | time: 16.8s | train_loss: 0.9903 train_acc: 64.80% | test_loss: 0.7425 test_acc: 72.54%\n",
            "Epoch 02/8 | time: 17.1s | train_loss: 0.5348 train_acc: 80.42% | test_loss: 0.5692 test_acc: 79.13%\n",
            "Epoch 03/8 | time: 17.3s | train_loss: 0.4301 train_acc: 84.69% | test_loss: 0.4928 test_acc: 82.08%\n",
            "Epoch 04/8 | time: 16.9s | train_loss: 0.3834 train_acc: 86.47% | test_loss: 0.3567 test_acc: 87.38%\n",
            "Epoch 05/8 | time: 16.9s | train_loss: 0.3449 train_acc: 87.77% | test_loss: 0.6237 test_acc: 77.50%\n",
            "Epoch 06/8 | time: 17.6s | train_loss: 0.3277 train_acc: 88.43% | test_loss: 0.5515 test_acc: 81.73%\n",
            "Epoch 07/8 | time: 16.7s | train_loss: 0.3089 train_acc: 89.16% | test_loss: 0.4082 test_acc: 84.74%\n",
            "Epoch 08/8 | time: 17.0s | train_loss: 0.2668 train_acc: 90.67% | test_loss: 0.2785 test_acc: 90.21%\n",
            "Best test accuracy for fashion_mnist: 90.21%\n",
            "---------- Dataset: stl10 ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.64G/2.64G [01:21<00:00, 32.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/8 | time: 7.0s | train_loss: 2.2102 train_acc: 20.24% | test_loss: 2.0604 test_acc: 25.84%\n",
            "Epoch 02/8 | time: 5.8s | train_loss: 1.8717 train_acc: 28.94% | test_loss: 1.7722 test_acc: 31.96%\n",
            "Epoch 03/8 | time: 6.8s | train_loss: 1.7002 train_acc: 32.78% | test_loss: 1.7693 test_acc: 31.48%\n",
            "Epoch 04/8 | time: 5.7s | train_loss: 1.6204 train_acc: 35.12% | test_loss: 1.6089 test_acc: 36.10%\n",
            "Epoch 05/8 | time: 7.0s | train_loss: 1.6005 train_acc: 35.22% | test_loss: 1.6150 test_acc: 36.45%\n",
            "Epoch 06/8 | time: 5.8s | train_loss: 1.5745 train_acc: 37.40% | test_loss: 1.6320 test_acc: 36.56%\n",
            "Epoch 07/8 | time: 6.8s | train_loss: 1.5726 train_acc: 37.94% | test_loss: 1.5300 test_acc: 40.01%\n",
            "Epoch 08/8 | time: 5.7s | train_loss: 1.5047 train_acc: 40.12% | test_loss: 1.4971 test_acc: 41.74%\n",
            "Best test accuracy for stl10: 41.74%\n",
            "---------- Dataset: svhn ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 182M/182M [00:15<00:00, 11.5MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100%|██████████| 64.3M/64.3M [00:02<00:00, 31.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/8 | time: 26.2s | train_loss: 1.9801 train_acc: 29.46% | test_loss: 2.0876 test_acc: 30.26%\n",
            "Epoch 02/8 | time: 26.4s | train_loss: 1.4343 train_acc: 50.58% | test_loss: 1.2850 test_acc: 56.07%\n",
            "Epoch 03/8 | time: 25.8s | train_loss: 1.0229 train_acc: 66.76% | test_loss: 0.9824 test_acc: 66.86%\n",
            "Epoch 04/8 | time: 25.8s | train_loss: 0.7723 train_acc: 75.69% | test_loss: 1.6822 test_acc: 47.28%\n",
            "Epoch 05/8 | time: 25.8s | train_loss: 0.6269 train_acc: 80.56% | test_loss: 0.7962 test_acc: 74.07%\n",
            "Epoch 06/8 | time: 26.0s | train_loss: 0.5403 train_acc: 83.43% | test_loss: 0.6996 test_acc: 77.14%\n",
            "Epoch 07/8 | time: 27.5s | train_loss: 0.4860 train_acc: 85.06% | test_loss: 0.6940 test_acc: 77.85%\n",
            "Epoch 08/8 | time: 26.4s | train_loss: 0.3938 train_acc: 88.31% | test_loss: 0.3608 test_acc: 89.81%\n",
            "Best test accuracy for svhn: 89.81%\n",
            "\n",
            "All done. Total time: 13.98 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"| Dataset | Test Accuracy (%) |\")\n",
        "for k, v in results.items():\n",
        "    acc_str = f\"{v:.2f}\" if (v is not None and not isinstance(v, str)) else \"Error / n/a\"\n",
        "    print(f\"| {k} | {acc_str} |\")\n"
      ],
      "metadata": {
        "id": "RKiZGe_pC1Ho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00e7eb9-efbe-4c3c-da4c-147726cf07ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Dataset | Test Accuracy (%) |\n",
            "| cifar10 | 70.85 |\n",
            "| mnist | 98.80 |\n",
            "| fashion_mnist | 90.21 |\n",
            "| stl10 | 41.74 |\n",
            "| svhn | 89.81 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8ydVoEF-DDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i5yubjO8-FnX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}